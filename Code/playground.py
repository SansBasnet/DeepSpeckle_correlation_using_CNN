# -*- coding: utf-8 -*-
"""Playground.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aXXpw7X_wkNYBphCKFTB6GIxdGFWARa4
"""

import pandas as pd
import numpy as np
import tensorflow as tf
import keras 
import os
import seaborn as sns
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from keras.utils import np_utils
from keras.preprocessing.image import ImageDataGenerator

#Connecting to google drive
from google.colab import drive
drive.mount('/content/drive')
#os.chdir('/content/drive/My Drive/Colab Notebooks/Spotify Data-X Project')

# Commented out IPython magic to ensure Python compatibility.
# %ls drive/'My Drive'

os.chdir('/content/drive/My Drive/Thesis')

df = pd.read_csv("SB2.csv", header=None)

#df += 1 # Adding 1 to all entries in df for scaling by log bc of scattering in the experiment. 
df

v = df.values
num_cols = v.shape[1]

im_1 = v[:,:1920]
im_1.shape

plt.imshow(im_1)

df = pd.read_csv("SB2.csv", header=None)
v = df.values
num_cols = df.shape[1]
sns.heatmap(df)
df =

images = []
for i in range(10):
    start_col = i * 1920
    end_col = (i+1) * 1920
    image = v[:,start_col:end_col]
    images.append(image)

images = np.array(images)

i = 9
plt.imshow(images[i,:,:])
plt.savefig('specke.png')

np.all(images[0,:,:] == images[8,:,:])

# checking if each samples is different
images[0,200,750], images[1,200,750]

X_data = images
X_data.shape

# generate labels
# call this class 2
y_data = np.ones(shape=(10,))
y_data = y_data * 2
y_data

# split into training/testing
X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3)
X_train.shape, X_test.shape

filename_labels = {
    "SB2.csv" : 0,
    "SB5.csv" : 1,
    "SB7.csv" : 2
}

def extract_images(array : np.ndarray, num_images):
    # takes in 2d array: (num_rows x (num_cols * num_images))
    images = []
    
    for i in range(num_images):
        start_col = i * 1920
        end_col = (i+1) * 1920
        image = array[:,start_col:end_col]
        images.append(image)
        
    images = np.array(images)
    
    return images # 3d array: (num_images x num_rows x num_cols)

def process_data(filename : str):
    df = pd.read_csv(filename, header=None)
    array = df.values # the raw CSV file data as an array
    num_images = int(array.shape[1] / 1920)
    
    images_in_class = extract_images(array, num_images) # 3d array: (num_images x num_rows x num_cols)
    X_data = images_in_class
    
    y_data = np.ones(shape=(num_images)) # array of ones, that is as long as the # of images we have
    label = filename_labels[filename]
    y_data = y_data * label
    
    return X_data, y_data

X_sb2, y_sb2 = process_data("SB2.csv")
X_sb5, y_sb5 = process_data("SB5.csv")
X_sb7, y_sb7 = process_data("SB7.csv")

# Combine the data into one array
X_data = np.concatenate((X_sb2, X_sb5, X_sb7))
X_data.shape

y_data = np.concatenate((y_sb2, y_sb5, y_sb7))
y_data.shape

X_data[0] # input for sample 0

y_data[0] # output/target for sample 0

# look at the data
sample_num = 10
plt.imshow(X_data[sample_num])
plt.title(f"Sample number {sample_num} (class {y_data[sample_num]})")
plt.show()

def one_hot_encode(y_data):
    # takes in 1d y data, return one-hot-encoded version
    encoded = np_utils.to_categorical(y_data, num_classes=len(filename_labels))
    return encoded

def split_training_testing(X_data, y_data, test_size):
    y_encoded = one_hot_encode(y_data)
    X_train, X_test, y_train, y_test = train_test_split(X_data, y_encoded, test_size=test_size, shuffle=True)
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = split_training_testing(X_data, y_data, test_size=0.3)

print(f"X_train has {len(X_train)} samples\nX_test has {len(X_test)} samples\ny_train has {len(y_train)} samples\ny_test has {len(y_test)} samples\n")

y_train

y_enc = one_hot_encode(y_data)
y_enc

def generate_model():
  model = tf.keras.Sequential([
      #first convolutional layer
      tf.keras.layers.Conv2D(32, filter_size=3, activation='relu'),
      tf.keras.layers.MaxPool2D(pool_size=2, strides=2),

      #second convolutional layer   
      tf.keras.layers.Conv2D(64, filter_size=3, activation='relu'),
      tf.keras.layers.MaxPool2D(pool_size=2, strides=2 ), 

      #fullt connected classifier
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(1024, activation='relu'),
      tf.keras.layers.Dense(3, activation = 'softmax') # 3 outputs         
  ])
  return model

  model.compile(optimizer = 'adam',
              loss = 'categorical_crossentropy',
              metrics = ['accuracy'])

  model.fit(X_train, y_train, epochs =10)

model = tf.keras.models.Sequential() #Sequential is a feed-forward
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu)) #128 neurons  
model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu)) 
model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu)) 
model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu)) 
model.add(tf.keras.layers.Dense(3, activation = tf.nn.softmax)) 

model.compile(optimizer = 'adam',
              loss = 'categorical_crossentropy',
              metrics = ['accuracy'])

model.fit(X_train, y_train, epochs =10)

model.summary()

val_loss, val_acc = model.evaluate(X_test, y_test)
print(val_loss, val_acc)

num_classes = 3

model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),
                 activation='relu'))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])

model.fit(X_train, y_train, epochs =10)

model.save('epic_num_reader.model')

new_model = tf.keras.models.load_model('epic_num_reader.model')

predictions = new_model.predict(_test)
print(predictions)

import numpy as np
print(np.argmax(predictions[8]))

import matplotlib.pyplot as plt
plt.imshow(X_test[8])
plt.show()

import matplotlib.pyplot as plt
print(X_train[0])

plt.imshow(X_train[0])
plt.show()

plt.imshow(X_train[0], cmap = plt.cm.binary)
plt.show()

